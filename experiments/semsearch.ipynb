{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logseq semantic search and retrieval \n",
    "A test notebook for semantic search and retrieval across a Logseq note database.\n",
    "Based on https://learn.deeplearning.ai/langchain-chat-with-your-data/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import openai\n",
    "from pathlib import Path\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "CHROMA_PERSIST_DIR = Path('../data/chroma')\n",
    "CHROMA_PERSIST_DIR.unlink(missing_ok=True)\n",
    "CHROMA_PERSIST_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the documents from the Logseq database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs\n",
    "import logging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "logseq_location = os.getenv(\"LOGSEQ_DIR\")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    logseq_location,  \n",
    "    glob=\"**/*.md\", \n",
    "    # FIXME: The Logseq data dir contains a 'logseq/bak' subdirectory that contains\n",
    "    # old versions of the files. Need to add a param to DirectoryLoader to exclude this directory.\n",
    "    # exclude_glob='logseq/bak/**/*.*', \n",
    "    # https://github.com/langchain-ai/langchain/pull/11831\n",
    "    loader_cls=UnstructuredMarkdownLoader, \n",
    "    silent_errors=True\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the documents with the splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=['\\n\\n', '\\n- ', '\\n', '\\.', ' ', ''])\n",
    "splits = splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed the documents into vectors and store them in a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=str(CHROMA_PERSIST_DIR),\n",
    ")\n",
    "vectordb.persist()\n",
    "docs = vectordb.similarity_search(\"What potential projects do I have?\", k=3)\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(\"What potential projects do I have?\", k=2, fetch_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a retrieval chain with the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "\tllm, retriever=vectordb.as_retriever(search_type='mmr')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'What potential projects do I have?',\n",
    "    'How to train a model?',\n",
    "    'Write a Wikipedia-style article about Iltar',\n",
    "]\n",
    "result = qa_chain({'query': questions[2]})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
